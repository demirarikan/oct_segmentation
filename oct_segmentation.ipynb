{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from oct_dataset import OCTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-87d3cacf9501202e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-87d3cacf9501202e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"input_size\": 1 * 1024 * 512,\n",
    "    \"in_channels\": 1,\n",
    "    \"out_channels\": 5,\n",
    "    \"device\": device,\n",
    "    \"epochs\": 10,\n",
    "    \"weight_decay\": 1e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OCTDataset(root_dir='data/train_data', transform=transforms)\n",
    "test_dataset = OCTDataset(root_dir='data/test_data', transform=transforms)\n",
    "val_dataset = OCTDataset(root_dir='data/val_data', transform=transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=hparams['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(dataloader):\n",
    "    for batch in dataloader:\n",
    "        images, masks = batch\n",
    "\n",
    "        image = images[0].permute(1, 2, 0).numpy()\n",
    "        mask = masks[0].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot the image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title('Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot the segmentation mask\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(mask, cmap='viridis', alpha=0.5)  # Adjust cmap based on your segmentation task\n",
    "        plt.title('Segmentation Mask')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "visualize_data(train_dataloader)\n",
    "visualize_data(test_dataloader)\n",
    "visualize_data(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [32, 32, 1024, 512]             320\n",
      "       BatchNorm2d-2        [32, 32, 1024, 512]              64\n",
      "              ReLU-3        [32, 32, 1024, 512]               0\n",
      "         MaxPool2d-4         [32, 32, 512, 256]               0\n",
      "            Conv2d-5         [32, 32, 512, 256]           9,248\n",
      "       BatchNorm2d-6         [32, 32, 512, 256]              64\n",
      "              ReLU-7         [32, 32, 512, 256]               0\n",
      "         MaxPool2d-8         [32, 32, 256, 128]               0\n",
      "            Conv2d-9         [32, 64, 256, 128]          18,496\n",
      "      BatchNorm2d-10         [32, 64, 256, 128]             128\n",
      "             ReLU-11         [32, 64, 256, 128]               0\n",
      "        MaxPool2d-12          [32, 64, 128, 64]               0\n",
      "           Conv2d-13          [32, 64, 128, 64]          36,928\n",
      "      BatchNorm2d-14          [32, 64, 128, 64]             128\n",
      "             ReLU-15          [32, 64, 128, 64]               0\n",
      "        MaxPool2d-16           [32, 64, 64, 32]               0\n",
      "           Conv2d-17          [32, 128, 64, 32]          73,856\n",
      "      BatchNorm2d-18          [32, 128, 64, 32]             256\n",
      "             ReLU-19          [32, 128, 64, 32]               0\n",
      "        MaxPool2d-20          [32, 128, 32, 16]               0\n",
      "           Conv2d-21          [32, 128, 32, 16]         147,584\n",
      "      BatchNorm2d-22          [32, 128, 32, 16]             256\n",
      "             ReLU-23          [32, 128, 32, 16]               0\n",
      "        MaxPool2d-24           [32, 128, 16, 8]               0\n",
      "           Conv2d-25           [32, 128, 16, 8]         147,584\n",
      "      BatchNorm2d-26           [32, 128, 16, 8]             256\n",
      "             ReLU-27           [32, 128, 16, 8]               0\n",
      "           Conv2d-28           [32, 128, 16, 8]         147,584\n",
      "      BatchNorm2d-29           [32, 128, 16, 8]             256\n",
      "             ReLU-30           [32, 128, 16, 8]               0\n",
      "           Conv2d-31            [32, 64, 16, 8]         147,520\n",
      "      BatchNorm2d-32            [32, 64, 16, 8]             128\n",
      "             ReLU-33            [32, 64, 16, 8]               0\n",
      "         Upsample-34           [32, 64, 32, 16]               0\n",
      "           Conv2d-35           [32, 64, 32, 16]          36,928\n",
      "      BatchNorm2d-36           [32, 64, 32, 16]             128\n",
      "             ReLU-37           [32, 64, 32, 16]               0\n",
      "         Upsample-38           [32, 64, 64, 32]               0\n",
      "           Conv2d-39           [32, 32, 64, 32]          36,896\n",
      "      BatchNorm2d-40           [32, 32, 64, 32]              64\n",
      "             ReLU-41           [32, 32, 64, 32]               0\n",
      "         Upsample-42          [32, 32, 128, 64]               0\n",
      "           Conv2d-43          [32, 32, 128, 64]           9,248\n",
      "      BatchNorm2d-44          [32, 32, 128, 64]              64\n",
      "             ReLU-45          [32, 32, 128, 64]               0\n",
      "         Upsample-46         [32, 32, 256, 128]               0\n",
      "           Conv2d-47         [32, 32, 256, 128]          18,464\n",
      "      BatchNorm2d-48         [32, 32, 256, 128]              64\n",
      "             ReLU-49         [32, 32, 256, 128]               0\n",
      "         Upsample-50         [32, 32, 512, 256]               0\n",
      "           Conv2d-51         [32, 32, 512, 256]           9,248\n",
      "      BatchNorm2d-52         [32, 32, 512, 256]              64\n",
      "             ReLU-53         [32, 32, 512, 256]               0\n",
      "         Upsample-54        [32, 32, 1024, 512]               0\n",
      "           Conv2d-55         [32, 5, 1024, 512]             165\n",
      "================================================================\n",
      "Total params: 841,989\n",
      "Trainable params: 841,989\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 64.00\n",
      "Forward/backward pass size (MB): 3534.00\n",
      "Params size (MB): 3.21\n",
      "Estimated Total Size (MB): 3601.21\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torchsummary\\torchsummary.py:93: RuntimeWarning: overflow encountered in scalar add\n",
      "  total_output += np.prod(summary[layer][\"output_shape\"])\n"
     ]
    }
   ],
   "source": [
    "from seg_model import UNet\n",
    "\n",
    "model = UNet(hparams[\"in_channels\"], hparams['out_channels'])\n",
    "model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 1024, 512), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Training step\n",
    "        for images, masks in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_dataloader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                \n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            val_loss /= len(val_dataloader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    " \n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, optimizer, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    loss_cutoff = len(train_loader) // 10\n",
    "    \n",
    "    # The scheduler is used to change the learning rate every few \"n\" steps.\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * len(train_loader) / 5), gamma=hparams.get('gamma', 0.8))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train() \n",
    "        \n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "        \n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() \n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "\n",
    "            pred = model(images)\n",
    "            loss = loss_func(pred, labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "            training_loss = training_loss[-loss_cutoff:]\n",
    "\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss)), \n",
    "                                      lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr'])\n",
    "            )\n",
    "\n",
    "            tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                pred = model(images)\n",
    "                loss = loss_func(pred, labels)\n",
    "                validation_loss.append(loss.item())\n",
    "\n",
    "                val_loop.set_postfix(val_loss = \"{:.8f}\".format(np.mean(validation_loss)))\n",
    "\n",
    "                tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    test_scores = []\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        targets_mask = (targets >= 0).cpu()\n",
    "        test_scores.append(np.mean((preds.cpu() == targets.cpu())[targets_mask].numpy()))\n",
    "\n",
    "    return np.mean(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams['learning_rate'], weight_decay=hparams['weight_decay'])\n",
    "path = os.path.join('logs')\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "tb_logger = SummaryWriter(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/10]:   0%|                                                                                                  | 0/116 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 10.28 GiB is allocated by PyTorch, and 84.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moct seg model v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, loss_func, tb_logger, optimizer, epochs, name)\u001b[0m\n\u001b[0;32m     29\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     30\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m---> 32\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(pred, labels) \n\u001b[0;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\seg_model.py:65\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     63\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_conv2(torch\u001b[38;5;241m.\u001b[39mcat([enc3, bottleneck], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     64\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_conv3(torch\u001b[38;5;241m.\u001b[39mcat([enc2, dec2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 65\u001b[0m dec4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_conv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43menc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(dec4)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# create segmentation map from 5 classes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torch\\nn\\modules\\upsampling.py:156\u001b[0m, in \u001b[0;36mUpsample.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dmrar\\Desktop\\JHU\\code\\oct_seg_model\\Lib\\site-packages\\torch\\nn\\functional.py:4020\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4014\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[0;32m   4015\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[0;32m   4016\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[0;32m   4017\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[0;32m   4018\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mupsample_bilinear2d_vec(\n\u001b[0;32m   4019\u001b[0m                 \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[1;32m-> 4020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bilinear2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4022\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 10.28 GiB is allocated by PyTorch, and 84.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_model(model=model, train_loader=train_dataloader, val_loader=val_dataloader, loss_func=loss, tb_logger=tb_logger, optimizer=optimizer, epochs=hparams['epochs'], name='oct seg model v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.98901151 0.54954473]\n",
      "   [0.2814473  0.07728957]]\n",
      "\n",
      "  [[0.4444695  0.47280797]\n",
      "   [0.048522   0.16332445]]\n",
      "\n",
      "  [[0.11595071 0.62739168]\n",
      "   [0.85618205 0.65010242]]\n",
      "\n",
      "  [[0.99072168 0.47035075]\n",
      "   [0.61829448 0.28266721]]\n",
      "\n",
      "  [[0.97600332 0.673068  ]\n",
      "   [0.44053089 0.28968734]]]]\n",
      "softmaxed:\n",
      " tensor([[[[0.2508, 0.1975],\n",
      "          [0.1628, 0.1580]],\n",
      "\n",
      "         [[0.1455, 0.1829],\n",
      "          [0.1290, 0.1722]],\n",
      "\n",
      "         [[0.1048, 0.2135],\n",
      "          [0.2893, 0.2802]],\n",
      "\n",
      "         [[0.2513, 0.1825],\n",
      "          [0.2280, 0.1941]],\n",
      "\n",
      "         [[0.2476, 0.2235],\n",
      "          [0.1909, 0.1954]]]], dtype=torch.float64)\n",
      "get the max: \n",
      " tensor([[[3, 4],\n",
      "         [2, 2]]])\n",
      "torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(45)\n",
    "input_data = np.random.rand(1, 5, 2, 2)\n",
    "print(input_data)\n",
    "softmaxed = torch.nn.functional.softmax(torch.tensor(input_data), dim=1)\n",
    "print('softmaxed:\\n', softmaxed)\n",
    "_, max = torch.max(softmaxed, dim=1)\n",
    "print('get the max: \\n', max)\n",
    "print(max.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oct_seg_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
