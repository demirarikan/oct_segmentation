{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from oct_dataset import OCTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: 32120: No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 32120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 32120), started 16:49:37 ago. (Use '!kill 32120' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-33ebdb643cf9442b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-33ebdb643cf9442b\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"input_size\": 1 * 1024 * 512,\n",
    "    \"in_channels\": 1,\n",
    "    \"out_channels\": 5,\n",
    "    \"device\": device,\n",
    "    \"epochs\": 50,\n",
    "    \"weight_decay\": 1e-6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OCTDataset(root_dir='data/train_data', transform=transforms)\n",
    "test_dataset = OCTDataset(root_dir='data/test_data', transform=transforms)\n",
    "val_dataset = OCTDataset(root_dir='data/val_data', transform=transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=hparams['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, masks in train_dataloader:\n",
    "    print(images.shape)\n",
    "    print(masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(dataloader):\n",
    "    for batch in dataloader:\n",
    "        images, masks = batch\n",
    "\n",
    "        image = images[0].permute(1, 2, 0).numpy()\n",
    "        mask = masks[0].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot the image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title('Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot the segmentation mask\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(mask, cmap='viridis', alpha=0.5)  # Adjust cmap based on your segmentation task\n",
    "        plt.title('Segmentation Mask')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "visualize_data(train_dataloader)\n",
    "visualize_data(test_dataloader)\n",
    "visualize_data(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seg_model import UNet\n",
    "\n",
    "model = UNet(hparams[\"in_channels\"], hparams['out_channels'])\n",
    "model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 1024, 512), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Training step\n",
    "        for images, masks in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_dataloader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                \n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            val_loss /= len(val_dataloader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path=r'D:\\Desktop\\demir\\oct_segmentation\\saved_models', name='default'):\n",
    "    version_folder_number = 1\n",
    "    while True:\n",
    "        version_folder = os.path.join(path, f'version{version_folder_number}')\n",
    "        if not os.path.exists(version_folder):\n",
    "            os.makedirs(version_folder)\n",
    "            break\n",
    "        version_folder_number += 1\n",
    "\n",
    "    # Save the file inside the test folder\n",
    "    save_path = os.path.join(version_folder, f'final_{name}.pth')\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    " \n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, optimizer, epochs=10, name=\"default\", save_path=r'D:\\Desktop\\demir\\oct_segmentation\\saved_models'):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    loss_cutoff = len(train_loader) // 10\n",
    "    \n",
    "    # The scheduler is used to change the learning rate every few \"n\" steps.\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * len(train_loader) / 5), gamma=hparams.get('gamma', 0.8))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train() \n",
    "        \n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "        \n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() \n",
    "            images, labels = batch\n",
    "            labels = labels.squeeze().long()\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "\n",
    "            pred = model(images)\n",
    "            loss = loss_func(pred, labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "            training_loss = training_loss[-loss_cutoff:]\n",
    "\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(np.mean(training_loss)), \n",
    "                                      lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr'])\n",
    "            )\n",
    "\n",
    "            tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                images, labels = batch\n",
    "                labels = labels.squeeze().long()\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                pred = model(images)\n",
    "                loss = loss_func(pred, labels)\n",
    "                validation_loss.append(loss.item())\n",
    "\n",
    "                val_loop.set_postfix(val_loss = \"{:.8f}\".format(np.mean(validation_loss)))\n",
    "\n",
    "                tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "        \n",
    "        if epoch % 10 == 0 and epoch != 0:\n",
    "            save_model_path = os.path.join(save_path, f'{name}_epoch_{epoch + 1}.pth')\n",
    "            torch.save(model.state_dict(), save_model_path)\n",
    "        \n",
    "    save_model(model, save_path, name)\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    test_scores = []\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        targets_mask = (targets >= 0).cpu()\n",
    "        test_scores.append(np.mean((preds.cpu() == targets.cpu())[targets_mask].numpy()))\n",
    "\n",
    "    return np.mean(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams['learning_rate']) #, weight_decay=hparams['weight_decay']\n",
    "path = os.path.join('logs')\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "tb_logger = SummaryWriter(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check make small dataset see if network can overfit\n",
    "index_list = [100, 32, 326]\n",
    "small_trainset = torch.utils.data.Subset(train_dataset, index_list)\n",
    "small_trainloader = torch.utils.data.DataLoader(small_trainset, batch_size=3, shuffle=True)\n",
    "small_valset = torch.utils.data.Subset(val_dataset, index_list)\n",
    "small_valloader = torch.utils.data.DataLoader(small_valset, batch_size=3, shuffle=True)\n",
    "\n",
    "trained_test_model = train_model(model=model, train_loader=small_trainloader, val_loader=small_valloader, loss_func=loss, tb_logger=tb_logger, optimizer=optimizer, epochs=10, name='sanity check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_model(model=model, train_loader=train_dataloader, val_loader=val_dataloader, loss_func=loss, tb_logger=tb_logger, optimizer=optimizer, epochs=40, name='oct_seg_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oct_seg_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
